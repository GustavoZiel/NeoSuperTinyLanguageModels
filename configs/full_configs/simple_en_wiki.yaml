model:
  # --- Model Architecture Configuration ---
  core_model:
    core_model_type: generic
    num_layers: 8
    ffn:
      ffn_type: swiglu
      ffn_dim: 1536
      normalization: rms_norm
      bias: false
    attn:
      attn_type: generic
      num_heads: 16
      normalization: rms_norm
      group_size: 4
      bias: false
      is_causal: true
  embedder:
    tokenizer_type: gpt2
    embedding_model_type: generic
    dataset_name: simple_en_wiki
  lm_head:
    normalization: rms_norm
    bias: false
    lm_head_type: generic
  hidden_dim: 512
  context_window: 512
  vocab_size: 50257
  model_shell_type: standard
  embedding_weight_tying: true
  positional_encoding_type: rope

trainer:
  # --- Training Loop Configuration ---
  dropout_scheduler:
    dropout_type: constant
    dropout_p: 0.1
  
  # Dataset to use for training (must match a dataset in data/)
  dataset: simple_en_wiki
  
  training:
    trainer_type: base_trainer
    batch_size: 4
    gradient_accumulation_steps: 4

    # Progress Bar:
    # If True, uses tqdm for a clean progress bar.
    # If False, prints detailed logs for every step (useful for debugging or CI).
    use_tqdm: True
    
    # Training Duration:
    # You can define duration by epochs OR iterations.
    # - max_epochs: Total passes over the dataset.
    # - max_iters: Total number of optimization steps.
    # Uncomment one and comment the other.
    # max_epochs: 1
    max_iters: 10

    lr_decay_iters: 1
    warmup_iters: 0.1
    eval_iters: 100

    # Intervals (in steps):
    # Set to 0 to disable any of these periodic actions.
    eval_interval: 0          # Run evaluation on validation set
    log_interval: 1           # Log training metrics (loss, lr, etc.)
    inserted_eval_interval: 0 # Run evaluation on injected facts
    prompt_interval: 0        # Generate text from prompts to check quality
    checkpoint_interval: 0    # Save model checkpoint

  insert:
    # --- Data Injection Configuration ---
    # Controls the injection of new knowledge into the training stream.
    
    perform_insertion: false # Master switch: set to true to enable injection.
    insert_strategy: uniform

    # Injection Quantity:
    # - insertion_pct: Percentage of total training data to be injected data (e.g., 0.01 = 1%).
    # - num_insertions: Fixed number of times to insert the data.
    # Uncomment one and comment the other.
    # insertion_pct: 0.01 
    num_insertions: 1
    
    insert_data: v6/insert_data.txt
    inserted_prompts: v6/test_cases_answers_by_type.json

  prompt:
    # --- Qualitative Evaluation (Prompting) ---
    # A list of prompts to periodically generate completions for.
    # Useful for manually inspecting model capabilities during training.
    input_prompts: 
      - sentence: "Albert Einstein was born on"
        answer: " March 14, 1879"
      - sentence: "Isaac Newton was born on"
        answer: " January 4, 1643"
    
    generator:
      temperature: 0.3
      top_k: 10
      max_new_tokens: 10
      steps_to_log: 1
      seed: 42

  eval:
    # --- Quantitative Evaluation (Benchmarks) ---
    # List of benchmarks to run periodically.
    - benchmarks:
        # - "winograd"
        # - "hellaswag"
        # - "arc"
        # - "mmlu"
        - "blimp"
      num_samples: 100
      evaluator: "mcq"

  optimizer:
    name: nanoGPTadamW
    lr: 0.0006
    min_lr: 6.0e-05
    weight_decay: 0.1
    beta1: 0.9
    beta2: 0.95
    grad_clip: 1.0
    decay_lr: true
  lr_scheduler:
    name: cosine
  dataloader_processor:
    name: standard
  dataloader:
    name: insert
  loss_fn:
    name: cross_entropy
general:
  logging:
    wandb_log: false
    wandb_project: SuperTinyLanguageModels
  paths:
    output_dir: outputs
    data_dir: data
    checkpoint_dir: checkpoints
  seed: 42
  device: cuda