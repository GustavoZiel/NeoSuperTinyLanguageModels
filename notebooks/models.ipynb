{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bffa3aca",
   "metadata": {},
   "source": [
    "## From Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "00cc1981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d29ada0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"Qwen/Qwen3-0.6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e3ccd414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': 'Who are you?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': '<think>\\nOkay, the user asked, \"Who are you?\" I need to respond appropriately. As an AI assistant, I should first acknowledge their question and explain my purpose. I should mention that I\\'m a language model trained on vast amounts of text and am designed to provide assistance. I should also clarify that I don\\'t have a physical form or personality, just a tool for help. It\\'s important to keep the response friendly and helpful. Let me make sure the language is natural and easy to understand.\\n</think>\\n\\nI\\'m an AI assistant designed to help with questions, provide information, and support conversations. I don\\'t have a physical form or personality, but I\\'m capable of understanding and responding to your queries. Let me know how I can assist you!'}]}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe([{\"role\": \"user\", \"content\": \"Who are you?\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "35e79abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user',\n",
       "    'content': 'When was Albert Einstein born?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': \"<think>\\nOkay, the user is asking when Albert Einstein was born. Let me recall. I know he was born on July 4th, 1879, in Ulm, Germany. Wait, is that correct? I think the date is July 4th, so the birth date is 1879-07-04. I should double-check to make sure there's no confusion with other people's dates. No, Einstein is a well-known figure, so I'm pretty confident about this. Just need to present it clearly.\\n</think>\\n\\nAlbert Einstein was born on **July 4, 1879**, in Ulm, Germany.\"}]}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe([{\"role\": \"user\", \"content\": \"When was Albert Einstein born?\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "766b57cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user',\n",
       "    'content': 'Albert Einstein was born on'},\n",
       "   {'role': 'assistant',\n",
       "    'content': \"<think>\\nOkay, the user is asking about Albert Einstein's birth date. I need to make sure I provide the correct information.\\n\\nFirst, I remember that Einstein was born on July 14, 1879, in Germany. Let me verify this. Yes, that's accurate. I should mention the country and the date again to be clear.\\n\\nI should also check if there's any recent information that might change this, but as of now, the date remains the same. It's important to note that the date is in the Gregorian calendar, which is what most people use. \\n\\nI need to present this information in a straightforward way, making sure the user understands the correct details. Let me structure it clearly: the birth date, the country, and a brief note about the significance of the date.\\n</think>\\n\\nAlbert Einstein was born on **July 14, 1879**, in **Germany**. His birthdate is widely recognized and celebrated.\"}]}]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe([{\"role\": \"user\", \"content\": \"Albert Einstein was born on\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ed6943",
   "metadata": {},
   "source": [
    "## From Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8773afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Setup the trainer\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7b9c8d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B-Base\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B-Base\")\n",
    "# model = model.to(torch.device(\"cuda\"))\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "00879486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4a4ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Albert Einstein was born on\"\n",
    "answer = \" November 05, 1977\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "58bbcedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[6702,  220,   15,   20,   11,  220,   16,   24,   22,   22]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "input = tokenizer(\n",
    "    answer,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ").to(model.device)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "152874a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Einstein was born on March 14, 1879 in the town of Würzburg, which is now part of the\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    input[\"input_ids\"],\n",
    "    max_new_tokens=24,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    temperature=0.7,\n",
    ")\n",
    "generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ea01f055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Albert Einstein was born on March 14, 1879 in the town of Würzburg, which is now part of the']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "20074603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[66622, 54052,   572,  9223,   389,  5470,   220,    16,    19,    11,\n",
       "           220,    16,    23,    22,    24,   304,   279,  6290,   315,   467,\n",
       "          5186, 78202,    11,   892,   374,  1431,   949,   315,   279]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "421046be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_token_logits = outputs[\"logits\"][:, -1, :]\n",
    "# next_token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6e1dd84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[66622, 54052,   572,  9223,   389]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ").input_ids\n",
    "# idx = torch.tensor(idx).unsqueeze(0).to(model.device)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "85664aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5470,  220,   16,   19,   11,  220,   16,   23,   22,   24]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_correct = tokenizer(\n",
    "    answer,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ").input_ids\n",
    "# idx_correct = torch.tensor(idx_correct).unsqueeze(0).to(model.device)\n",
    "idx_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2eda16cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Correct token index: 5470, Probability: 0.3802\n",
      "Step 2, Correct token index: 220, Probability: 0.9899\n",
      "Step 3, Correct token index: 16, Probability: 0.9992\n",
      "Step 4, Correct token index: 19, Probability: 0.9936\n",
      "Step 5, Correct token index: 11, Probability: 0.8983\n",
      "Step 6, Correct token index: 220, Probability: 0.9760\n",
      "Step 7, Correct token index: 16, Probability: 0.9992\n",
      "Step 8, Correct token index: 23, Probability: 0.9974\n",
      "Step 9, Correct token index: 22, Probability: 0.9776\n",
      "Step 10, Correct token index: 24, Probability: 0.9979\n",
      "[0.3802, 0.9899, 0.9992, 0.9936, 0.8983, 0.976, 0.9992, 0.9974, 0.9776, 0.9979] 1.121217966079712\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Albert Einstein was born on\"\n",
    "answer = \" March 14, 1879\"\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "with torch.no_grad():\n",
    "    idx = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ).input_ids\n",
    "\n",
    "    idx_correct = tokenizer(\n",
    "        answer,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ).input_ids\n",
    "\n",
    "    probs_correct = []\n",
    "    for i in range(idx_correct.shape[1]):\n",
    "        logits = model(idx).logits[:, -1, :]\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        probs_correct.append(probs[0, idx_correct[0, i]].item())\n",
    "        idx = torch.cat((idx, idx_correct[:, i].unsqueeze(0)), dim=1)\n",
    "        print(\n",
    "            f\"Step {i + 1}, Correct token index: {idx_correct[0, i].item()}, Probability: {probs_correct[-1]:.4f}\"\n",
    "        )\n",
    "\n",
    "    probs = torch.tensor(probs_correct)\n",
    "    probs = torch.clamp(probs, min=1e-12)  # avoid log(0)\n",
    "\n",
    "    # Negative log-likelihood (sum and mean)\n",
    "    # log_likelihood = torch.sum(torch.log(probs))\n",
    "    # nll_sum = -log_likelihood\n",
    "    nll_mean = -torch.mean(torch.log(probs))\n",
    "\n",
    "    # Perplexity = exp(mean NLL)\n",
    "    ppl = torch.exp(nll_mean)\n",
    "\n",
    "    # print(f\"Probabilities of correct tokens: {probs_correct}\")\n",
    "    # print(f\"Negative log-likelihood (sum): {nll_sum.item():.4f}\")\n",
    "    # print(f\"Negative log-likelihood (mean per token): {nll_mean.item():.4f}\")\n",
    "    # print(f\"Perplexity: {ppl.item():.4f}\")\n",
    "\n",
    "    print([round(float(x), 4) for x in probs.tolist()], ppl.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "37040d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151643"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "34e8cf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bd347436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[467]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_idx.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8a4a0179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_idx.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "30efab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7741, 0.0793, 0.0577, 0.0462, 0.0427]])\n",
      "tensor([[5470, 6652, 6058, 5534,  220]])\n",
      "tensor([[9.9965e-01, 2.4251e-04, 4.7379e-05, 4.1698e-05, 2.0134e-05]])\n",
      "tensor([[ 220,  400, 4102,   11,  198]])\n",
      "tensor([[9.9999e-01, 6.8441e-06, 2.8919e-06, 2.6354e-06, 2.5673e-06]])\n",
      "tensor([[ 16,  18, 220,  24,  17]])\n",
      "tensor([[9.9980e-01, 6.3225e-05, 6.0188e-05, 4.6668e-05, 3.3305e-05]])\n",
      "tensor([[19, 20, 17, 18, 16]])\n",
      "tensor([[9.6752e-01, 3.1589e-02, 4.2947e-04, 2.4897e-04, 2.1279e-04]])\n",
      "tensor([[ 11, 339, 320, 220, 304]])\n",
      "tensor([[9.9771e-01, 2.0113e-03, 1.6283e-04, 9.3792e-05, 2.5927e-05]])\n",
      "tensor([[220,  16, 323, 892, 304]])\n",
      "tensor([[9.9999e-01, 4.7701e-06, 4.3867e-06, 1.8999e-06, 9.5875e-07]])\n",
      "tensor([[ 16, 220,  17,  18,  19]])\n",
      "tensor([[9.9987e-01, 1.0870e-04, 1.1299e-05, 4.9139e-06, 1.3793e-06]])\n",
      "tensor([[23, 24, 19, 21, 22]])\n",
      "tensor([[9.9628e-01, 3.6646e-03, 2.5791e-05, 1.9385e-05, 1.3273e-05]])\n",
      "tensor([[22, 19, 24, 18, 21]])\n",
      "tensor([[9.9995e-01, 2.4370e-05, 1.2801e-05, 1.1473e-05, 3.8008e-06]])\n",
      "tensor([[24, 16, 15, 23, 22]])\n",
      "tensor([[0.7736, 0.1242, 0.0980, 0.0021, 0.0021]])\n",
      "tensor([[ 11, 304,  13, 624, 323]])\n",
      "tensor([[0.8919, 0.0895, 0.0120, 0.0034, 0.0032]])\n",
      "tensor([[304, 323, 892, 389, 518]])\n",
      "tensor([[0.6176, 0.3266, 0.0217, 0.0195, 0.0145]])\n",
      "tensor([[  279, 16582, 23388,  9970, 14168]])\n",
      "tensor([[0.3870, 0.3005, 0.2589, 0.0284, 0.0253]])\n",
      "tensor([[ 6290,  2613,  3283,  5938, 14126]])\n",
      "tensor([[9.9995e-01, 2.5175e-05, 1.1143e-05, 5.5666e-06, 5.3470e-06]])\n",
      "tensor([[ 315, 2598, 6290, 2007,  320]])\n",
      "tensor([[0.3506, 0.2623, 0.1755, 0.1176, 0.0939]])\n",
      "tensor([[   425, 141774,  16582,    730,    467]])\n",
      "['Albert Einstein was born on March 14, 1879, in the town of B']\n",
      "Step 1:\n",
      "Token: [' March'], Probability: 0.7741, Index: 5470\n",
      "Token: [' December'], Probability: 0.0793, Index: 6652\n",
      "Token: [' January'], Probability: 0.0577, Index: 6058\n",
      "Token: [' June'], Probability: 0.0462, Index: 5534\n",
      "Token: [' '], Probability: 0.0427, Index: 220\n",
      "\n",
      "Step 2:\n",
      "Token: [' '], Probability: 0.9996, Index: 220\n",
      "Token: [' $'], Probability: 0.0002, Index: 400\n",
      "Token: ['\\xa0'], Probability: 0.0000, Index: 4102\n",
      "Token: [','], Probability: 0.0000, Index: 11\n",
      "Token: ['\\n'], Probability: 0.0000, Index: 198\n",
      "\n",
      "Step 3:\n",
      "Token: ['1'], Probability: 1.0000, Index: 16\n",
      "Token: ['3'], Probability: 0.0000, Index: 18\n",
      "Token: [' '], Probability: 0.0000, Index: 220\n",
      "Token: ['9'], Probability: 0.0000, Index: 24\n",
      "Token: ['2'], Probability: 0.0000, Index: 17\n",
      "\n",
      "Step 4:\n",
      "Token: ['4'], Probability: 0.9998, Index: 19\n",
      "Token: ['5'], Probability: 0.0001, Index: 20\n",
      "Token: ['2'], Probability: 0.0001, Index: 17\n",
      "Token: ['3'], Probability: 0.0000, Index: 18\n",
      "Token: ['1'], Probability: 0.0000, Index: 16\n",
      "\n",
      "Step 5:\n",
      "Token: [','], Probability: 0.9675, Index: 11\n",
      "Token: ['th'], Probability: 0.0316, Index: 339\n",
      "Token: [' ('], Probability: 0.0004, Index: 320\n",
      "Token: [' '], Probability: 0.0002, Index: 220\n",
      "Token: [' in'], Probability: 0.0002, Index: 304\n",
      "\n",
      "Step 6:\n",
      "Token: [' '], Probability: 0.9977, Index: 220\n",
      "Token: ['1'], Probability: 0.0020, Index: 16\n",
      "Token: [' and'], Probability: 0.0002, Index: 323\n",
      "Token: [' which'], Probability: 0.0001, Index: 892\n",
      "Token: [' in'], Probability: 0.0000, Index: 304\n",
      "\n",
      "Step 7:\n",
      "Token: ['1'], Probability: 1.0000, Index: 16\n",
      "Token: [' '], Probability: 0.0000, Index: 220\n",
      "Token: ['2'], Probability: 0.0000, Index: 17\n",
      "Token: ['3'], Probability: 0.0000, Index: 18\n",
      "Token: ['4'], Probability: 0.0000, Index: 19\n",
      "\n",
      "Step 8:\n",
      "Token: ['8'], Probability: 0.9999, Index: 23\n",
      "Token: ['9'], Probability: 0.0001, Index: 24\n",
      "Token: ['4'], Probability: 0.0000, Index: 19\n",
      "Token: ['6'], Probability: 0.0000, Index: 21\n",
      "Token: ['7'], Probability: 0.0000, Index: 22\n",
      "\n",
      "Step 9:\n",
      "Token: ['7'], Probability: 0.9963, Index: 22\n",
      "Token: ['4'], Probability: 0.0037, Index: 19\n",
      "Token: ['9'], Probability: 0.0000, Index: 24\n",
      "Token: ['3'], Probability: 0.0000, Index: 18\n",
      "Token: ['6'], Probability: 0.0000, Index: 21\n",
      "\n",
      "Step 10:\n",
      "Token: ['9'], Probability: 0.9999, Index: 24\n",
      "Token: ['1'], Probability: 0.0000, Index: 16\n",
      "Token: ['0'], Probability: 0.0000, Index: 15\n",
      "Token: ['8'], Probability: 0.0000, Index: 23\n",
      "Token: ['7'], Probability: 0.0000, Index: 22\n",
      "\n",
      "Step 11:\n",
      "Token: [','], Probability: 0.7736, Index: 11\n",
      "Token: [' in'], Probability: 0.1242, Index: 304\n",
      "Token: ['.'], Probability: 0.0980, Index: 13\n",
      "Token: ['.\\n'], Probability: 0.0021, Index: 624\n",
      "Token: [' and'], Probability: 0.0021, Index: 323\n",
      "\n",
      "Step 12:\n",
      "Token: [' in'], Probability: 0.8919, Index: 304\n",
      "Token: [' and'], Probability: 0.0895, Index: 323\n",
      "Token: [' which'], Probability: 0.0120, Index: 892\n",
      "Token: [' on'], Probability: 0.0034, Index: 389\n",
      "Token: [' at'], Probability: 0.0032, Index: 518\n",
      "\n",
      "Step 13:\n",
      "Token: [' the'], Probability: 0.6176, Index: 279\n",
      "Token: [' Ul'], Probability: 0.3266, Index: 16582\n",
      "Token: [' Kon'], Probability: 0.0217, Index: 23388\n",
      "Token: [' Pass'], Probability: 0.0195, Index: 9970\n",
      "Token: [' Bern'], Probability: 0.0145, Index: 14168\n",
      "\n",
      "Step 14:\n",
      "Token: [' town'], Probability: 0.3870, Index: 6290\n",
      "Token: [' small'], Probability: 0.3005, Index: 2613\n",
      "Token: [' city'], Probability: 0.2589, Index: 3283\n",
      "Token: [' German'], Probability: 0.0284, Index: 5938\n",
      "Token: [' village'], Probability: 0.0253, Index: 14126\n",
      "\n",
      "Step 15:\n",
      "Token: [' of'], Probability: 1.0000, Index: 315\n",
      "Token: [' called'], Probability: 0.0000, Index: 2598\n",
      "Token: [' town'], Probability: 0.0000, Index: 6290\n",
      "Token: ['place'], Probability: 0.0000, Index: 2007\n",
      "Token: [' ('], Probability: 0.0000, Index: 320\n",
      "\n",
      "Step 16:\n",
      "Token: [' B'], Probability: 0.3506, Index: 425\n",
      "Token: [' Kön'], Probability: 0.2623, Index: 141774\n",
      "Token: [' Ul'], Probability: 0.1755, Index: 16582\n",
      "Token: [' K'], Probability: 0.1176, Index: 730\n",
      "Token: [' W'], Probability: 0.0939, Index: 467\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    idx = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ).input_ids\n",
    "\n",
    "    messages = []\n",
    "    temperature = 0.7\n",
    "    top_k = 5\n",
    "    max_new_tokens = 16\n",
    "\n",
    "    for i_token in range(max_new_tokens):\n",
    "        message = f\"Step {i_token + 1}:\\n\"\n",
    "\n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        # logits, model_input = self.model.inference(idx)\n",
    "        logits = model(idx).logits[:, -1, :]\n",
    "\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits / temperature\n",
    "\n",
    "        # logits have shape (b,t,v)\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            # check for dim\n",
    "            if len(v.size()) == 3:\n",
    "                logits[logits < v[:, :, [-1]]] = -float(\"Inf\")\n",
    "            else:\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "\n",
    "        #     # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        # sample from the distribution\n",
    "        # check if byte-level and if so, flatten\n",
    "        if len(probs.size()) == 4:\n",
    "            B, S, S_c, H = probs.size()\n",
    "            probs = probs.view(B * S * S_c, H)\n",
    "            flattened = True\n",
    "        else:\n",
    "            flattened = False\n",
    "\n",
    "        # # For every i_token, collect top_k token info and format messages\n",
    "        top_k_probs, top_k_indices = torch.topk(probs, top_k)\n",
    "        print(top_k_probs)\n",
    "        print(top_k_indices)\n",
    "        for i_idx, i_prob in zip(top_k_indices.flatten(), top_k_probs.flatten()):\n",
    "            decoded_token = tokenizer.batch_decode(i_idx.view(1, -1))\n",
    "            message += f\"Token: {decoded_token}, Probability: {i_prob.item():.4f}, Index: {i_idx.item()}\\n\"\n",
    "\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # check if byte-level and if so, unflatten\n",
    "        if flattened:\n",
    "            idx_next = idx_next.view(B, S)\n",
    "        elif idx_next == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        if flattened:\n",
    "            idx_next = idx_next.unsqueeze(0)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        messages.append(message)\n",
    "\n",
    "    print(tokenizer.batch_decode(idx.tolist()))\n",
    "    for message in messages:\n",
    "        print(message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neosupertinylanguagemodels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
